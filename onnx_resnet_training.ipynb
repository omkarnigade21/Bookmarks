{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0ba9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c36fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ecda01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__  # should be 0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20bdbe01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'data',\n",
       " 'cats_and_dogs-1.ipynb',\n",
       " 'best_model.pth',\n",
       " 'vit_model.onnx',\n",
       " 'model-repository',\n",
       " 'cropped-frames-2.zip',\n",
       " 'cropped-frames-2',\n",
       " 'suhana_Horizontal_Vertical.ipynb',\n",
       " 'blue channel.png',\n",
       " 'green channel.png',\n",
       " 'red channel.png',\n",
       " 'frame1_image-1.png',\n",
       " 'hsv-frame1.png',\n",
       " 'data-hor-ver',\n",
       " 'horizontal',\n",
       " 'test.py',\n",
       " 'image-classification-by-resnet-50.ipynb',\n",
       " 'onnx.ipynb',\n",
       " 'onnx(1).ipynb',\n",
       " 'onnx_resnet.ipynb',\n",
       " 'valve',\n",
       " 'scoop_v1.pt',\n",
       " 'valve.pt',\n",
       " 'Untitled.ipynb',\n",
       " 'best_vit_model.pt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = \"valve/train_v1/\"\n",
    "train_dir = 'valve/train_v1/train'   # Update paths\n",
    "val_dir = 'valve/train_v1/val'\n",
    "import os \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd47bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e342ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    'train': \n",
    "    datasets.ImageFolder(input_path + 'train', data_transforms['train']),\n",
    "    'validation': \n",
    "    datasets.ImageFolder(input_path + 'val', data_transforms['validation'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03d55275",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=128,\n",
    "                                shuffle=True,\n",
    "                                num_workers=0),  # for Kaggle\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                batch_size=64,\n",
    "                                shuffle=False,\n",
    "                                num_workers=0)  # for Kaggle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b018bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83cde0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True).to(device)\n",
    "    \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model.fc = nn.Sequential(nn.Linear(2048, 128),nn.ReLU(inplace=True),nn.Linear(128, 2)).to(device)# add number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b633bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1aea2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=3):\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):                                                                            \n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_loss = running_loss / len(image_datasets[phase])\n",
    "                epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "    \n",
    "    \n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5baccc8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "----------\n",
      "train loss: 0.6248, acc: 0.6074\n",
      "validation loss: 0.5992, acc: 0.5000\n",
      "Epoch 2/8\n",
      "----------\n",
      "train loss: 0.4691, acc: 0.7516\n",
      "validation loss: 0.4492, acc: 0.8943\n",
      "Epoch 3/8\n",
      "----------\n",
      "train loss: 0.2473, acc: 0.9252\n",
      "validation loss: 0.3467, acc: 0.8984\n",
      "Epoch 4/8\n",
      "----------\n",
      "train loss: 0.1674, acc: 0.9653\n",
      "validation loss: 0.4179, acc: 0.7724\n",
      "Epoch 5/8\n",
      "----------\n",
      "train loss: 0.1310, acc: 0.9772\n",
      "validation loss: 0.1743, acc: 0.8984\n",
      "Epoch 6/8\n",
      "----------\n",
      "train loss: 0.0822, acc: 0.9848\n",
      "validation loss: 0.1074, acc: 0.9512\n",
      "Epoch 7/8\n",
      "----------\n",
      "train loss: 0.0643, acc: 0.9924\n",
      "validation loss: 0.1004, acc: 0.9390\n",
      "Epoch 8/8\n",
      "----------\n",
      "train loss: 0.0514, acc: 0.9935\n",
      "validation loss: 0.0912, acc: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0179, acc: 0.9933\n",
      "validation loss: 0.0157, acc: 1.0000\n",
      "Epoch 90/100\n",
      "----------\n",
      "train loss: 0.0186, acc: 0.9933\n",
      "validation loss: 0.0202, acc: 0.9949\n",
      "Epoch 91/100\n",
      "----------\n",
      "train loss: 0.0249, acc: 0.9906\n",
      "validation loss: 0.0147, acc: 1.0000\n",
      "Epoch 92/100\n",
      "----------\n",
      "train loss: 0.0314, acc: 0.9893\n",
      "validation loss: 0.0092, acc: 1.0000\n",
      "Epoch 93/100\n",
      "----------\n",
      "train loss: 0.0285, acc: 0.9893\n",
      "validation loss: 0.0152, acc: 1.0000\n",
      "Epoch 94/100\n",
      "----------\n",
      "train loss: 0.0340, acc: 0.9879\n",
      "validation loss: 0.0531, acc: 0.9646\n",
      "Epoch 95/100\n",
      "----------\n",
      "train loss: 0.0306, acc: 0.9933\n",
      "validation loss: 0.0040, acc: 1.0000\n",
      "Epoch 96/100\n",
      "----------\n",
      "train loss: 0.0270, acc: 0.9919\n",
      "validation loss: 0.0551, acc: 0.9646\n",
      "Epoch 97/100\n",
      "----------\n",
      "train loss: 0.0213, acc: 0.9919\n",
      "validation loss: 0.0036, acc: 1.0000\n",
      "Epoch 98/100\n",
      "----------\n",
      "train loss: 0.0232, acc: 0.9919\n",
      "validation loss: 0.0218, acc: 0.9949\n",
      "Epoch 99/100\n",
      "----------\n",
      "train loss: 0.0184, acc: 0.9946\n",
      "validation loss: 0.0084, acc: 1.0000\n",
      "Epoch 100/100\n",
      "----------\n",
      "train loss: 0.0196, acc: 0.9960\n",
      "validation loss: 0.0101, acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, num_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bebd720",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained.state_dict(), 'valve.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0ee94d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=False).to(device)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 2), nn.Softmax(1) ).to(device)# add number of classes \n",
    "model.load_state_dict(torch.load('valve.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57831bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_img_paths = [\"data-hor-ver/val/horizontal.horizontal_folder2_frame2_image-6.png\",\"data-hor-ver/val/vertical.vertical_folder6_frame1_image-196.png\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c51704a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: OFF (1.00 confidence)\n"
     ]
    }
   ],
   "source": [
    "img_list = [Image.open( img_path) for img_path in validation_img_paths]\n",
    "img_list\n",
    "model.eval()\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load image\n",
    "# img_path = \"/mnt/data/folder10_frame1_image-2157.png\"\n",
    "# image = Image.open(img_path).convert(\"RGB\")\n",
    "input_tensor = transform(img_list[0]).unsqueeze(0).to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    confidence = torch.max(output).item()\n",
    "\n",
    "# Define class labels (change as per your dataset)\n",
    "class_labels = ['OFF', 'ON']  # adjust as needed\n",
    "\n",
    "# Print result\n",
    "print(f\"Prediction: {class_labels[predicted_class]} ({confidence:.2f} confidence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b71f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = torch.stack([data_transforms['validation'](img).to(device)\n",
    "                                for img in img_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9201e871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9458, 0.0542],\n",
       "        [0.0332, 0.9668]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_logits_tensor = model(validation_batch)\n",
    "pred_logits_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60bf96b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('train', <torch.utils.data.dataloader.DataLoader object at 0x7fe7f198d160>), ('validation', <torch.utils.data.dataloader.DataLoader object at 0x7fe7f198d640>)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pred_probs = pred_logits_tensor.cpu().data.numpy()\n",
    "\n",
    "\n",
    "dataloaders.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce797503",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m ax \u001b[38;5;241m=\u001b[39m axs[i]\n\u001b[1;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mpred_probs[i,\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mpred_probs[i,\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mpred_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mpred_probs[i,\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m      6\u001b[0m ax\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, len(img_list), figsize=(5, 5))\n",
    "for i, img in enumerate(img_list):\n",
    "    ax = axs[i]\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"{:.0f}% {:.0f}% {:.0f}% {:.0f}%\".format(100*pred_probs[i,0],100*pred_probs[i,1],100*pred_probs[i,2],100*pred_probs[i,3]))\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = [ \"input\" ]\n",
    "output_names = [ \"output\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52919b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.autograd.Variable(torch.randn(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, dummy_input.cuda(), 'scoop.onnx', input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a8577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
